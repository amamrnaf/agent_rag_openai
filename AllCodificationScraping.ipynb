{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chapiters = [f\"{i:02}\" if i < 10 else str(i) for i in range(1, 100)]\n",
    "print(chapiters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.adapters import Retry\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def generate_urls(base_url, prefixes):\n",
    "    urls = [f\"{base_url}{prefix}\" for prefix in prefixes]\n",
    "    return urls\n",
    "\n",
    "def scrape_href_tags_with_session(url):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "    }\n",
    "    print(f\"Fetching URL: {url}\")\n",
    "    session = requests.Session()\n",
    "    retry = Retry(connect=3, backoff_factor=0.5)\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    session.mount('http://', adapter)\n",
    "    session.mount('https://', adapter)\n",
    "    response = session.get(url, headers=headers)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find all 'a' tags (anchor tags) and get their 'href' attributes\n",
    "        href_tags = [a.get('href') for a in soup.find_all('a') if a.get('href')]\n",
    "\n",
    "        return href_tags\n",
    "    else:\n",
    "        print(\"Failed to retrieve the webpage.\")\n",
    "\n",
    "# Generate URLs for different prefixes\n",
    "base_url = 'https://www.douane.gov.ma/adil/ta_chap.asp?recherche1='\n",
    "prefixes = chapiters # Add more prefixes as needed\n",
    "urls = generate_urls(base_url, prefixes)\n",
    "\n",
    "# Scrape href tags for each URL\n",
    "href_tags_dict = {}\n",
    "for i, url in enumerate(urls, start=1):\n",
    "    href_tags = scrape_href_tags_with_session(url)\n",
    "    href_tags_dict[f\"href_tags{i}\"] = href_tags\n",
    "\n",
    "# Access the href_tags lists later using href_tags1, href_tags2, etc.\n",
    "# print(href_tags_dict['href_tags1'])  # Access the href_tags from the first URL\n",
    "# print(href_tags_dict['href_tags2'])  # Access the href_tags from the second URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_position(href_tags_list):\n",
    "    positions = []\n",
    "    for href_tag in href_tags_list:\n",
    "        position = href_tag.split('position=')[-1]\n",
    "        positions.append(position)\n",
    "    return positions\n",
    "\n",
    "positions_dict = {}\n",
    "for key, href_tags_list in href_tags_dict.items():\n",
    "    positions_dict[key] = extract_position(href_tags_list)\n",
    "\n",
    "# # Print the positions and their lengths for each href_tags list\n",
    "# for key, positions in positions_dict.items():\n",
    "#     print(f\"Positions for {key}: {positions}\")\n",
    "#     print(f\"Number of positions: {len(positions)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_positions(positions):\n",
    "    filtered_positions = [pos for pos in positions if len(pos) == 10]\n",
    "    return filtered_positions\n",
    "\n",
    "filtered_positions_dict = {}\n",
    "for key, positions_list in positions_dict.items():\n",
    "    filtered_positions_dict[key] = filter_positions(positions_list)\n",
    "\n",
    "# # Print the filtered positions and their lengths for each positions list\n",
    "# for key, filtered_positions in filtered_positions_dict.items():\n",
    "#     print(f\"Filtered Positions for {key}: {filtered_positions}\")\n",
    "#     print(f\"Number of filtered positions: {len(filtered_positions)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(input_list):\n",
    "    unique_list = list(set(input_list))\n",
    "    return unique_list\n",
    "\n",
    "# Create a dictionary to store unique lists for each filtered positions list\n",
    "unique_lists_dict = {}\n",
    "\n",
    "# Loop through filtered_positions_dict and apply the remove_duplicates function to each list\n",
    "for i, (key, filtered_positions_list) in enumerate(filtered_positions_dict.items(), start=1):\n",
    "    unique_lists_dict[f'unique_list{i}'] = remove_duplicates(filtered_positions_list)\n",
    "\n",
    "    # If you want to explicitly create variables, uncomment the line below\n",
    "    # locals()[f'unique_list{i}'] = remove_duplicates(filtered_positions_list)\n",
    "\n",
    "    # Print the unique lists and their lengths for each filtered positions list\n",
    "    print(f\"Unique List for {key}: {unique_lists_dict[f'unique_list{i}']}\")\n",
    "    print(f\"Number of unique elements: {len(unique_lists_dict[f'unique_list{i}'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(unique_lists_dict[f'unique_list{99}'])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
